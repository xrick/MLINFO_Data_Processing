
# import pandas as pd
import json
# import re
from typing import Any, Dict, Optional
from pathlib import Path
import csv
# import argparse

import logging

# å°å…¥çˆ¶é¡åˆ¥
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from parsebase import ParseBase

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class CSVParser2(ParseBase):

    def __init__(self):
        super().__init__()
        self.rawcsv = None
        self.datalist = None
        self._rules = None
        self._rules_file = Path(__file__).parent / "rules.json"
        self.headers = None
        self.model_count = 0
        self.model_type = None
        self.processed_result = None
        self.default_output_path = None

    def beforeParse(self, data: Any, config: Optional[Dict] = None) -> bool:
        """
        è§£æå‰æº–å‚™å·¥ä½œ
        
        Args:
            data: CSV æª”æ¡ˆè·¯å¾‘æˆ– DataFrame
            config: è§£æé…ç½®åƒæ•¸
            
        Returns:
            bool: æº–å‚™å·¥ä½œæ˜¯å¦æˆåŠŸ
        """
        try:
            # è¨­ç½®è¼¸å…¥è³‡æ–™
            self.rawcsv = data
            
            # è¼‰å…¥è§£æè¦å‰‡
            self._rules = self._load_rules()
            if not self._rules:
                logger.error("ç„¡æ³•è¼‰å…¥è§£æè¦å‰‡")
                return False
            
            # è¼‰å…¥ CSV
            self.datalist = self._load_csv(self.rawcsv)
            
            # è¨­ç½®æ¨™é¡Œ
            self.headers = [
                rule.get("column_name", f"æ¬„ä½{i+1}")
                for i, rule in enumerate(self._rules[1])
            ]
            
            # è¨­ç½®æ¨¡å‹åƒæ•¸
            self.model_count = self._rules[0][0]["model_count"]
            self.model_type = self._rules[0][0]["model_type"]
            self.default_output_path = self._rules[0][0].get("default_output_path", "./output.csv")
            
            logger.info(f"è§£æå‰æº–å‚™å®Œæˆ - æ¨¡å‹æ•¸é‡: {self.model_count}, é¡å‹: {self.model_type}")
            return True
            
        except Exception as e:
            logger.error(f"è§£æå‰æº–å‚™å¤±æ•—: {str(e)}")
            return False
         

    def inParse(self):
        """
        ä¸»è¦è§£æé‚è¼¯
        
        Returns:
            List[Dict]: è§£æçµæœåˆ—è¡¨
        """
        self.collect_results()
        return self.processed_result

    
    def endParse(self) -> bool:
        """
        è§£æå¾Œè™•ç†å·¥ä½œ
        
        Returns:
            bool: å¾Œè™•ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            if self.processed_result:
                self.write_csv()
                logger.info("è§£æå¾Œè™•ç†å®Œæˆ")
                return True
            else:
                logger.error("ç„¡è§£æçµæœå¯è™•ç†")
                return False
        except Exception as e:
            logger.error(f"è§£æå¾Œè™•ç†å¤±æ•—: {str(e)}")
            return False
        
    
    def _load_rules(self):
        with open(self._rules_file, "r", encoding="utf-8") as f:
            return json.load(f)
        


    def _load_csv(self, file_path):
        with open(file_path, mode='r', encoding='utf-8-sig') as f:
            return list(csv.reader(f))



    def collect_results(self):
        result_rows = [[] for _ in range(self.model_count)]

        for rule_index, rule in enumerate(self._rules[1]):
            keywords = rule.get("keywords", [])
            logic = rule.get("logic", "OR").upper()
            rowspan = rule.get("rowspan", 1)
            column_name = rule.get("column_name", f"æ¬„ä½{rule_index+1}")

            matched_blocks = []

            for i, row in enumerate(self.datalist):
                if logic == "AND":
                    match = True
                    for kw in keywords:
                        if not any(kw.lower() in str(cell).lower() for cell in row):
                            match = False
                            break
                else:
                    match = False
                    for kw in keywords:
                        if any(kw.lower() in str(cell).lower() for cell in row):
                            match = True
                            break

                if match:
                    block = self.datalist[i:i + rowspan]
                    matched_blocks.append((i, block))

            if len(matched_blocks) == 0:
                print(f"âš ï¸ è¦å‰‡ {rule_index+1} - {column_name}: æ‰¾ä¸åˆ°é—œéµå­— {keywords}ï¼Œå…¨æ¬„å¡«ç©ºç™½")
                for idx in range(self.model_count):
                    result_rows[idx].append("")
            else:
                if len(matched_blocks) > 1:
                    print(f"âš ï¸ è­¦å‘Šï¼šè¦å‰‡ {rule_index+1} - {column_name} æ‰¾åˆ°è¶…éä¸€å€‹åŒ¹é…ï¼ˆå…± {len(matched_blocks)} å€‹ï¼‰ï¼Œåƒ…ä½¿ç”¨ç¬¬ä¸€ç­†ï¼")
                first_index, block = matched_blocks[0]
                print(f"ğŸ” è¦å‰‡ {rule_index+1} - {column_name}: æ‰¾åˆ°é—œéµå­— {keywords} æ–¼ç¬¬ {first_index+1} è¡Œ")

                for idx in range(self.model_count):
                    col_index = 2 + idx
                    if all(col_index < len(row) for row in block):
                        lines = []
                        for r in block:
                            value = r[col_index].strip()
                            prefix_label = r[1].strip() if len(r) > 1 else ""
                            if prefix_label:
                                lines.append(f"{prefix_label}: {value}")
                            else:
                                lines.append(value)
                        result = "\n".join(lines).strip()
                        result_rows[idx].append(result)
                        print(f"  â†’ æ©Ÿç¨®{idx+1}: {result}")
                    else:
                        result_rows[idx].append("")
                        print(f"  â†’ æ©Ÿç¨®{idx+1}: âš ï¸è­¦å‘Šï¼šè©²æ©Ÿç¨®æ­¤è™•ç„¡è³‡æ–™")

        self.processed_result=result_rows
    

    # def write_csv(self, output_path, result_rows, headers, model_type):
    def write_csv(self):
        headers = ["modeltype"] + self.headers
        with open(self.default_output_path, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.writer(f)
            writer.writerow(headers)
            for row in self.processed_result:
                writer.writerow([self.model_type] + row)
        print(f"âœ… å·²è¼¸å‡ºè‡³ï¼š{self.default_output_path}")

# def main():
#     parser = argparse.ArgumentParser(description="æ ¹æ“š JSON è¦å‰‡æœå°‹ CSV ä¸¦è½‰ç‚ºæ¯æ©Ÿç¨®ä¸€åˆ—æ ¼å¼ï¼ˆv4 + prefix æ¯åˆ—è®€å–ï¼‰")
#     parser.add_argument("csv_path", help="è¼¸å…¥CSV")
#     parser.add_argument("json_path", help="è¦å‰‡JSON")
#     parser.add_argument("--model_count", type=int, required=True, help="å¹¾å€‹æ©Ÿç¨®")
#     parser.add_argument("--model_type", type=str, required=True, help="å…±ç”¨çš„ model type å­—ä¸²")
#     parser.add_argument("--output_csv", required=True, help="è¼¸å‡ºCSVæª”å")

#     args = parser.parse_args()

#     reader = load_csv(args.csv_path)
#     rules = load_rules(args.json_path)
#     results = collect_results(reader, rules, args.model_count)

#     headers = [
#         rule.get("column_name").strip() if rule.get("column_name") and rule.get("column_name").strip() != "" else f"æ¬„ä½{i+1}"
#         for i, rule in enumerate(rules)
#     ]
#     write_csv(args.output_csv, results, headers, args.model_type)

# if __name__ == "__main__":
#     main()